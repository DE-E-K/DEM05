TMDB MOVIE DATA ANALYSIS - SETUP & EXECUTION GUIDE
==================================================

This guide walks you through setting up and running the complete TMDB data analysis pipeline.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 1: ENVIRONMENT SETUP (First Time Only)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1.1: Verify Prerequisites
------------------------------
Ensure you have:
âœ“ Python 3.8 or higher: python --version
âœ“ Java (for Spark): java -version
âœ“ Git (optional): git --version
âœ“ At least 2GB free disk space

Step 1.2: Clone/Navigate to Project
------------------------------------
Windows:
  cd "C:\path\to\Spark-impl"

Linux/Mac:
  cd /path/to/Spark-impl

Step 1.3: Create Virtual Environment
-------------------------------------
Windows:
  python -m venv venv
  venv\Scripts\activate

Linux/Mac:
  python3 -m venv venv
  source venv/bin/activate

Step 1.4: Install Dependencies
-------------------------------
pip install --upgrade pip
pip install -r requirements.txt

This installs:
  - pyspark          (3.5+)        Apache Spark for distributed processing
  - pandas           (2.0+)        Data manipulation
  - matplotlib       (3.7+)        Visualization
  - seaborn          (0.13+)       Statistical plots
  - aiohttp          (3.8+)        Async HTTP client
  - requests         (2.31+)       HTTP client
  - python-dotenv    (1.0+)        Environment variable management
  - pyarrow          (13.0+)       Columnar data format

Expected time: 3-5 minutes

Step 1.5: Configure API Credentials
------------------------------------
1. Go to https://www.themoviedb.org/settings/api
2. Sign up and get your API key
3. Create a .env file in the project root (Spark-impl/)
4. Add: TMDB_API_KEY=your_actual_api_key_here
5. Save the file

Example .env file:
  TMDB_API_KEY=abc123def456ghi789jkl012mno345pqr


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 2: PIPELINE EXECUTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Option A: RUN COMPLETE PIPELINE (Recommended)
----------------------------------------------
python main.py

This executes all 4 steps automatically:
  1. API Data Ingestion     (~30-60 seconds)
  2. Spark ETL              (~20-40 seconds)
  3. KPI Analysis           (~10-20 seconds)
  4. Visualization          (~5-10 seconds)

Total time: ~1-2 minutes

Expected output:
  âœ“ Started: Fetching Movie Data from API
  âœ“ Completed: Spark ETL Transformation
  âœ“ Completed: KPI Analysis Report
  âœ“ Completed: Visualization Generation

  ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY! ğŸ‰

Option B: RUN INDIVIDUAL STEPS (For Debugging)
-----------------------------------------------

Step 2A: Fetch Data from TMDB API
  python -m model.ingestion.fetch_data
  Output: data/raw/batch_*.json
  Time: 30-60 seconds

Step 2B: Transform with Spark ETL
  python -m model.processing.etl
  Output: data/processed/release_year=*/
  Time: 20-40 seconds

Step 2C: Generate KPI Report
  python -m model.analytics.kpi
  Output: output/kpi_analysis.txt
  Time: 10-20 seconds

Step 2D: Create Visualizations
  python -c "from model.visualization.plots import create_all_visualizations; import pandas as pd; df = pd.read_parquet('data/processed'); create_all_visualizations(df, 'output/plots')"
  Output: output/plots/*.png
  Time: 5-10 seconds


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 3: EXPLORE RESULTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

After pipeline completes, view your results:

3.1: KPI Analysis Report
------------------------
cat output/kpi_analysis.txt

Contains:
  âœ“ General statistics (total movies, avg rating, etc.)
  âœ“ Top 5 movies by revenue, profit, rating, popularity
  âœ“ Worst movies & lowest ROI
  âœ“ Advanced query results (Bruce Willis, Tarantino, Uma Thurman)
  âœ“ Franchise vs Standalone comparison
  âœ“ Top franchises analysis
  âœ“ Top directors ranking

3.2: Visual Analysis (Jupyter Notebook)
----------------------------------------
jupyter notebook notebooks/analysis.ipynb

Features:
  âœ“ Interactive data exploration
  âœ“ Custom filters and queries
  âœ“ Dynamic plotting
  âœ“ Real-time statistics

Cells included:
  1. Data Loading & Setup
  2. Exploratory Data Analysis (EDA)
  3. Advanced Filtering & Search Queries
  4. Franchise vs Standalone Analysis
  5. Top Directors Analysis
  6. Genre Performance
  7. Key Insights & Conclusions

3.3: Generated Visualizations
-----------------------------
Open images in: output/plots/

Eight plots created:
  1. revenue_vs_budget.png
     â†’ Scatter plot showing budget vs revenue relationship
     â†’ Color-coded by release year, sized by vote count

  2. roi_by_genre.png
     â†’ Box plot distribution of ROI across genres
     â†’ Shows profitability variance by genre

  3. popularity_vs_rating.png
     â†’ Correlation between audience popularity and critical ratings
     â†’ Color intensity indicates revenue

  4. yearly_trends.png
     â†’ Time series of average revenue, budget, and profit
     â†’ Identifies financial trends over years

  5. franchise_vs_standalone.png
     â†’ Four-subplot comparison of franchise vs standalone movies
     â†’ Analyzes revenue, budget, rating, and count

  6. rating_distribution.png
     â†’ Histogram of movie ratings with mean/median markers
     â†’ Shows quality distribution of dataset

  7. top_directors.png
     â†’ Horizontal bar chart of top 10 directors by revenue
     â†’ Includes movie count per director

  8. genre_performance.png
     â†’ Three-panel analysis: rating, revenue, and count by genre
     â†’ Identifies best-performing genres

3.4: Access Raw Data
---------------------
Processed movie data in Parquet format:
  data/processed/release_year=*/

Load in Python:
  import pandas as pd
  df = pd.read_parquet('data/processed')
  df.head()  # View first rows

Or in PySpark:
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[*]").getOrCreate()
  df = spark.read.parquet("data/processed")
  df.show()


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 4: UNDERSTANDING THE DATA
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Dataset Overview
---------------
Movies analyzed: 19 major films (as of last run)
Date range: 1997-2022
File format: Parquet (partitioned by release year)

Key Columns in Processed Data:
  id                      - Unique TMDB identifier
  title                   - Movie name
  release_year            - Year of release (partition key)
  budget_musd             - Budget in millions USD
  revenue_musd            - Revenue in millions USD
  profit                  - Revenue - Budget (in millions USD)
  roi                     - Return on Investment (Revenue / Budget)
  vote_average            - Critical/audience rating (0-10, from TMDB)
  vote_count              - Number of votes received
  popularity              - Popularity score (from TMDB)
  genres                  - Comma-separated genres (e.g., "Action|Adventure")
  belongs_to_collection   - Franchise name (null if standalone)
  director                - Primary director
  cast                    - Lead actors (pipe-separated)
  production_companies    - Studio names (pipe-separated)
  production_countries    - Countries involved (pipe-separated)
  spoken_languages        - Languages in film (pipe-separated)
  runtime                 - Duration in minutes
  overview                - Plot description

Data Quality Notes:
  âœ“ Null/0 budgets/revenues converted to NaN
  âœ“ Dates standardized to datetime
  âœ“ Text fields cleaned of placeholders
  âœ“ Nested structures flattened
  âœ“ "Released" status movies only


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 5: TROUBLESHOOTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Common Issues & Solutions
--------------------------

Issue: "error: TMDB_API_KEY is missing"
Solution: 
  1. Create .env file in project root
  2. Add: TMDB_API_KEY=your_api_key
  3. Save and restart the script

Issue: "ModuleNotFoundError: No module named 'pyspark'"
Solution:
  1. Verify virtual environment is activated
  2. Run: pip install -r requirements.txt
  3. Wait for installation to complete

Issue: "java.io.IOException: java.nio.file.NoSuchFileException: C:\hadoop\bin\winutils.exe"
Solution:
  The pipeline falls back to Pandas/PyArrow automatically.
  If issues persist:
    a) Download winutils.exe to C:\hadoop\bin\
    b) Or run on WSL/Linux instead

Issue: "Connection timeout" / "API Error 429"
Solution:
  The script includes retry logic with exponential backoff.
  If still failing:
    1. Check internet connection
    2. Verify TMDB API is operational
    3. Check rate limits (typically 40 requests/10 seconds)
    4. Wait 10-15 minutes and try again

Issue: "MemoryError" or "Spark OutOfMemory"
Solution:
  1. Close other applications
  2. Reduce concurrent requests: Edit config.py, set INGESTION_CONCURRENCY = 3
  3. Or increase available memory

Issue: "No visualizations generated"
Solution:
  1. Verify processed data exists: check data/processed/
  2. Ensure matplotlib/seaborn are installed: pip install matplotlib seaborn
  3. Check plots directory: ls output/plots/
  4. Try manual generation in Jupyter notebook

Checking Logs
--------------
If errors occur, check detailed logs:

  output/logs/ingestion.log    - API fetching details
  output/logs/etl.log          - Spark transformation steps
  output/logs/analytics.log    - Analysis execution
  output/logs/project.log      - General workflow

View recent logs:
  tail -50 output/logs/project.log


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 6: PROJECT STRUCTURE REFERENCE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Core Modules
-----------

model/config.py
  â†’ Configuration constants
  â†’ Directory paths
  â†’ API settings
  â†’ Edit this to customize behavior

model/logger.py
  â†’ Logging setup
  â†’ Creates timestamped logs

model/ingestion/fetch_data.py
  â†’ Async TMDB API client
  â†’ Rate limiting with semaphore
  â†’ Exponential backoff retry logic
  â†’ Newline-delimited JSON output

model/processing/schemas.py
  â†’ Spark DataFrame schemas
  â†’ Defines movie data structure
  â†’ Maps TMDB JSON to Spark types

model/processing/etl.py
  â†’ Main Spark transformations
  â†’ Data cleaning logic
  â†’ Column calculations (ROI, profit, etc.)
  â†’ Parquet write with partitioning

model/analytics/kpi.py
  â†’ KPI calculations and metrics
  â†’ Advanced filtering queries
  â†’ Franchise analysis
  â†’ Director analysis
  â†’ Report generation

model/visualization/plots.py
  â†’ matplotlib/seaborn plots
  â†’ Eight different visualization types
  â†’ Creates high-resolution PNG images

Main Execution
--------------
main.py
  â†’ Orchestrates entire pipeline
  â†’ Manages execution order
  â†’ Displays progress and summary
  â†’ Entry point for users

notebooks/analysis.ipynb
  â†’ Interactive analysis environment
  â†’ Jupyter notebook format
  â†’ Cell-by-cell execution
  â†’ Custom plotting and filtering


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
NEXT STEPS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

After successfully running the pipeline:

1. ANALYZE RESULTS
   â†’ Read KPI report: cat output/kpi_analysis.txt
   â†’ Identify patterns and insights
   â†’ Note key metrics

2. EXPLORE INTERACTIVELY
   â†’ Run Jupyter: jupyter notebook notebooks/analysis.ipynb
   â†’ Create custom queries
   â†’ Generate additional plots

3. UNDERSTAND THE CODE
   â†’ Review model/ modules
   â†’ Study Spark transformations
   â†’ Learn data engineering patterns

4. EXTEND FUNCTIONALITY
   â†’ Add new KPI calculations
   â†’ Create custom visualizations
   â†’ Implement additional filters

5. SHARE INSIGHTS
   â†’ Export findings to reports
   â†’ Present visualizations
   â†’ Document conclusions


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ¨ Ready to start? Run: python main.py

Questions? Check README_COMPREHENSIVE.md for detailed documentation.
