# ğŸ¬ TMDB Movie Data Analysis Using Apache Spark

> This project is a scalable data ingestion pipeline for fetching movie metadata from the TMDB API. It uses asynchronous I/O for high performance, bounded concurrency to respect rate limits by semaphore-based rate limiting, and a policy-driven retry mechanism with exponential backoff to handle transient failures safely. Data is written incrementally in a crash-safe format, making the pipeline reliable, resumable, and easy to operate, while its clean, modular design keeps it simple to maintain and extend as data volume grows.
---

## ğŸ“‘ Table of Contents

- [ğŸ¯ Quick Start](#-quick-start)
- [ğŸ—ï¸ Pipeline Architecture](#-pipeline-architecture)
- [ğŸ“Š Project Features](#-project-features)
- [ğŸ“ Directory Structure](#-directory-structure)
- [ğŸš€ Execution Guide](#-execution-guide)
- [ğŸ“ˆ Data Processing Pipeline](#-data-processing-pipeline)
- [ğŸ“Š Analysis Capabilities](#-analysis-capabilities)
- [ğŸ› ï¸ Configuration](#-configuration)
- [ğŸ” Logs & Monitoring](#-logs--monitoring)
- [âš™ï¸ Advanced Usage](#-advanced-usage)
- [ğŸ› Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Quick Start

### âœ… Prerequisites
- âœ“ Python 3.13+
- âœ“ Java 17+ (for Spark)
- âœ“ Spark 4.1.1

### âœ… Setup & Run (30 seconds)

```bash
# 1. Navigate to project
cd Spark-impl

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies (2-3 minutes)
pip install -r requirements.txt

# 4. Configure API key
echo TMDB_API_KEY=your_api_key_here > .env

# 5. Run complete pipeline
python main.py
```

**Expected output**: âœ“ Pipeline completes in ~1-2 minutes

---

## ğŸ—ï¸ Pipeline Architecture

### Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TMDB MOVIE ANALYSIS PIPELINE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    STEP 1: INGESTION          STEP 2: ETL               STEP 3: ANALYSIS
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         (Python)              (Apache Spark)           (Spark SQL)
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   TMDB API  â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Raw JSON    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Spark ETL      â”‚
    â”‚  (Async)    â”‚        â”‚  (Batches)   â”‚        â”‚  Transforms     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                        â”‚
         â”‚ Rate Limiting         â”‚ data/raw/              â”‚ data/processed/
         â”‚ Retry Logic           â”‚ batch_*.json           â”‚ release_year=*/
         â”‚ 19 Movies             â”‚                        â”‚
         â”‚ 5 Concurrent          â”‚                        â”‚ âœ“ Clean Data
         â”‚                       â”‚                        â”‚ âœ“ Schema  
         â”‚                       â”‚ âœ“ Deduplicate          â”‚ âœ“ Partition
         â”‚                       â”‚ âœ“ Validate             â”‚ âœ“ Optimize
         â”‚                       â”‚ âœ“ Error Handle         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   KPI Analysis   â”‚
                    â”‚  Report + 8 Vis  â”‚
                    â”‚  (Matplotlib)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚           â”‚
                    â”Œâ”€â”€â”€â”´â”€â”€â”    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â–¼      â–¼    â–¼          â–¼
              Report.txt  ğŸ“Š plots/      ğŸ““ Notebook
             (7 Sections) (8 Images)    (Interactive)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUTS: KPI Report | 8 Visualizations | Interactive Notebook | Full Logs   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture Components

| Stage | Technology | Input | Output | Responsibility |
|-------|-----------|-------|--------|-----------------|
| **Ingestion** | Python AsyncIO + aiohttp | TMDB API | JSON files | Fetch 19 movies with rate limiting & retry |
| **ETL** | Apache Spark 4.1.1 | JSON files | Parquet (partitioned) | Clean, transform, enrich data |
| **Analytics** | Spark SQL | Parquet | KPI report | Generate 7 analysis sections |
| **Visualization** | Matplotlib + Seaborn | Parquet | 8 PNG images | Create publication-ready plots |
| **Orchestration** | Python | All modules | Complete pipeline | Orchestrate & monitor execution |

---

## ğŸ“Š Project Features

### âœ… Data Engineering
- âœ“ **Async API Ingestion** - Non-blocking concurrent requests with semaphore rate limiting
- âœ“ **Error Handling** - Exponential backoff, retry logic, graceful fallbacks
- âœ“ **Schema Enforcement** - Explicit Spark schemas with nested data support
- âœ“ **Data Partitioning** - Optimized by release_year for query performance
- âœ“ **Windows Compatibility** - Hadoop workarounds + Pandas/PyArrow fallback

### âœ… Analytics & KPI
- âœ“ **Revenue Analysis** - Top movies by revenue, budget, profit
- âœ“ **ROI Metrics** - Return on investment calculations with thresholds
- âœ“ **Ratings Analysis** - Highest/lowest rated, most voted movies
- âœ“ **Advanced Queries** - Bruce Willis Sci-Fi, Tarantino + Uma Thurman
- âœ“ **Franchise Impact** - Comparative performance (franchise vs standalone)
- âœ“ **Director Rankings** - Top performers by revenue & ratings
- âœ“ **Genre Trends** - Performance analysis across 8+ genres

### âœ… Visualizations (8 High-Resolution Plots)
1. âœ“ **Revenue vs Budget** - Scatter with release year & vote count
2. âœ“ **ROI by Genre** - Box plot distribution analysis
3. âœ“ **Popularity vs Rating** - Correlation scatter plot
4. âœ“ **Yearly Trends** - Time series (revenue, budget, profit)
5. âœ“ **Franchise vs Standalone** - 4-subplot comparison
6. âœ“ **Rating Distribution** - Histogram with mean/median
7. âœ“ **Top Directors** - Horizontal bar chart ranking
8. âœ“ **Genre Performance** - Triple analysis panel

### âœ… Monitoring & Logging
- âœ“ **Component Logs** - Separate logs for ingestion, ETL, analytics
- âœ“ **Progress Tracking** - Real-time execution status
- âœ“ **Error Details** - Full stack traces in logs
- âœ“ **Performance Metrics** - Execution times for each step

---

## ğŸ“ Directory Structure

```
Spark-impl/
â”œâ”€â”€ ğŸ“„ main.py                          # â† RUN THIS: Orchestration script
â”œâ”€â”€ ğŸ“„ README.md                        # â† You are here
â”œâ”€â”€ ğŸ“Š requirements.txt                 # Dependencies
â”œâ”€â”€ ğŸ” .env                             # API key (create this)
â”‚
â”œâ”€â”€ ğŸ“ model/                           # Core pipeline modules
â”‚   â”œâ”€â”€ config.py                       # âœ“ Configuration constants
â”‚   â”œâ”€â”€ logger.py                       # âœ“ Logging setup
â”‚   â”œâ”€â”€ ingestion/                      # Stage 1: API Fetching
â”‚   â”‚   â””â”€â”€ fetch_data.py               # âœ“ Async TMDB client
â”‚   â”œâ”€â”€ processing/                     # Stage 2: Spark ETL
â”‚   â”‚   â”œâ”€â”€ etl.py                      # âœ“ Transformations
â”‚   â”‚   â””â”€â”€ schemas.py                  # âœ“ Spark schemas
â”‚   â”œâ”€â”€ analytics/                      # Stage 3: Analytics
â”‚   â”‚   â””â”€â”€ kpi.py                      # âœ“ KPI generation
â”‚   â””â”€â”€ visualization/                  # Stage 4: Plotting
â”‚       â””â”€â”€ plots.py                    # âœ“ 8 visualizations
â”‚
â”œâ”€â”€ ğŸ“ data/                            # Data directories
â”‚   â”œâ”€â”€ raw/                            # â† API output (JSON)
â”‚   â””â”€â”€ processed/                      # â† ETL output (Parquet)
â”‚
â”œâ”€â”€ ğŸ“ output/                          # Pipeline outputs
â”‚   â”œâ”€â”€ kpi_analysis.txt                # â† KPI report (7 sections)
â”‚   â”œâ”€â”€ logs/                           # Execution logs
â”‚   â”‚   â”œâ”€â”€ ingestion.log               # API fetch logs
â”‚   â”‚   â”œâ”€â”€ etl.log                     # Spark transform logs
â”‚   â”‚   â””â”€â”€ analytics.log               # Analysis logs
â”‚   â””â”€â”€ plots/                          # â† 8 visualizations (PNG)
â”‚
â””â”€â”€ ğŸ“ notebooks/
    â””â”€â”€ analysis.ipynb                  # â† Interactive analysis
```

---

## ğŸš€ Execution Guide

### Option 1: Run Complete Pipeline (Recommended)  [main orchestration script](main.py)

```bash
python main.py
```

**What happens:**
1. âœ“ Fetches 19 movies from TMDB (async, ~3 sec)
2. âœ“ Transforms with Spark ETL (20-40 sec)
3. âœ“ Generates KPI analysis (10-20 sec)
4. âœ“ Creates 8 visualizations (5-10 sec)

**Total time:** ~1-2 minutes

**Outputs generated:**
- âœ“ `output/kpi_analysis.txt` - Detailed report
- âœ“ `output/plots/` - 8 PNG visualizations
- âœ“ `output/logs/` - Complete execution logs

### Option 2: Run Individual Steps (For Debugging)

```bash
# Fetch data only
python -m model.ingestion.fetch_data

# ETL only (requires existing raw data)
python -m model.processing.etl

# Generate KPI report
python -m model.analytics.kpi
 c
# Create visualizations
python -c "from model.visualization.plots import create_all_visualizations; import pandas as pd; df = pd.read_parquet('data/processed'); create_all_visualizations(df, 'output/plots')"
```

### Option 3: Interactive Analysis

```bash
# Launch Jupyter notebook
jupyter notebook notebooks/analysis.ipynb
```

Features:
- âœ“ Load processed data
- âœ“ Run custom queries
- âœ“ Generate adhoc plots
- âœ“ Explore correlations

---

## ğŸ“ˆ Data Processing Pipeline

### [Stage 1: API Ingestion](model/ingestion/fetch_data.py)

**Purpose:** âœ“ Fetch movie data from TMDB

```python
Input:  19 movie IDs from config
Output: data/raw/batch_*.json
Time:   30-60 seconds
```

**Features:**
- âœ“ Async requests with asyncio
- âœ“ Rate limiting (5 concurrent)
- âœ“ Exponential backoff on 429
- âœ“ Retry logic (5 max retries)
- âœ“ Newline-delimited JSON

### [Stage 2: Spark ETL](model/processing/etl.py)

**Purpose:** âœ“ Clean and transform raw data

```python
Input:  data/raw/batch_*.json
Output: data/processed/release_year=*/
Time:   20-40 seconds
```

**Transformations:**
- âœ“ Filter "Released" movies
- âœ“ Parse dates â†’ extract year
- âœ“ Handle nulls/zeros â†’ NaN
- âœ“ Extract genres (pipe-separated)
- âœ“ Flatten credits â†’ cast/director
- âœ“ Calculate profit & ROI
- âœ“ Partition by release_year

### [Stage 3: KPI Analysis](model/analytics/kpi.py)

**Purpose:** âœ“ Generate business insights

```python
Input:  data/processed/
Output: output/kpi_analysis.txt
Time:   10-20 seconds
```

**7 Analysis Sections:**
1. âœ“ General Statistics
2. âœ“ Top Performing Movies
3. âœ“ Critical & Audience Ratings
4. âœ“ Advanced Filtering Queries
5. âœ“ Franchise vs Standalone
6. âœ“ Most Successful Franchises
7. âœ“ Top Directors

### [Stage 4: Visualizations](model/visualization/plots.py)

**Purpose:** âœ“ Create publication-ready plots

```python
Input:  data/processed/
Output: output/plots/*.png (8 images)
Time:   5-10 seconds
```

**8 Plots Generated:**
1. âœ“ Revenue vs Budget
2. âœ“ ROI by Genre
3. âœ“ Popularity vs Rating
4. âœ“ Yearly Trends
5. âœ“ Franchise vs Standalone
6. âœ“ Rating Distribution
7. âœ“ Top Directors
8. âœ“ Genre Performance

---

## ğŸ“Š Analysis Capabilities

### KPI Metrics

| Metric | Purpose | Calculation |
|--------|---------|-------------|
| **Revenue** | Total sales | Sum of gross revenue |
| **Profit** | Net earnings | Revenue - Budget |
| **ROI** | Return on investment | Revenue / Budget |
| **Rating** | Quality measure | TMDB average (0-10) |
| **Popularity** | Public interest | TMDB popularity index |
| **Franchise Impact** | Series performance | Grouped by collection |

### Advanced Queries

âœ“ **Bruce Willis Sci-Fi Action Query**
```python
Films with: Genre="Sci-Fi" AND Genre="Action" AND Cast="Bruce Willis"
Sorted by: Rating (highest first)
```

âœ“ **Tarantino + Uma Thurman Query**
```python
Films with: Director="Quentin Tarantino" AND Cast="Uma Thurman"
Sorted by: Runtime (shortest first)
```

âœ“ **Franchise vs Standalone Comparison**
```
Metrics: Revenue, Budget, Rating, Popularity, ROI
Grouped: belongs_to_collection NOT NULL vs NULL
```

---

## ğŸ› ï¸ Configuration

### Edit `model/config.py` to customize:

```python
# API Settings
TARGET_MOVIE_IDS = [...]           # âœ“ Movies to fetch
INGESTION_CONCURRENCY = 5          # âœ“ Concurrent requests
INGESTION_BATCH_SIZE = 100         # âœ“ Movies per batch file

# Spark Settings
SPARK_MASTER = "local[*]"          # âœ“ Use all cores
SPARK_APP_NAME = "TMDB_Analytics"  # âœ“ Application name

# Output Settings
FINAL_COLUMNS_ORDER = [...]        # âœ“ Column ordering
PLOTS_DIR = "output/plots"         # âœ“ Plot directory
KPI_REPORT_PATH = "output/kpi_analysis.txt"  # âœ“ Report path
```

### Create `.env` for API Key:

```bash
TMDB_API_KEY=your_api_key_from_themoviedb_org
```

Get your key: https://www.themoviedb.org/settings/api

---

## ğŸ” [Logs & Monitoring](output/logs/)


### View Execution Logs

```bash
# Real-time tail
tail -f output/logs/project.log

# View specific component
cat output/logs/ingestion.log
cat output/logs/etl.log
cat output/logs/analytics.log
```

### Log Files Generated

| File | Purpose | Size |
|------|---------|------|
| `project.log` | Overall workflow | ~10-50 KB |
| `ingestion.log` | API fetching | ~5-20 KB |
| `etl.log` | Spark transforms | ~20-100 KB |
| `analytics.log` | Analysis steps | ~10-30 KB |

---

## âš™ï¸ Advanced Usage

### Running on Different Platforms

**Linux/Mac:**
```bash
python main.py
# No Hadoop issues, runs natively
```

**Windows (with WSL2 recommended):**
```bash
# Option 1: Use WSL2
wsl python main.py

# Option 2: Native (with Hadoop workaround)
python main.py  # Falls back to Pandas/PyArrow automatically
```

### Increase Available Memory

```bash
export SPARK_DRIVER_MEMORY=4g
export SPARK_EXECUTOR_MEMORY=4g
python main.py
```

### Reduce Concurrent Requests

Edit `model/config.py`:
```python
INGESTION_CONCURRENCY = 3  # Reduce if rate-limited
```

### Access Raw Data in Python

```python
import pandas as pd

# Load processed data
df = pd.read_parquet('data/processed')

# Inspect columns
print(df.columns)
print(df.head())

# Custom analysis
df[df['genres'].str.contains('Action')].groupby('director')['revenue_musd'].sum()
```

---

## ğŸ› Troubleshooting

### âŒ "TMDB_API_KEY not found"

âœ… **Solution:**
```bash
# Create .env file with your API key
echo TMDB_API_KEY=your_key_here > .env
```

### âŒ "No JSON files in data/raw/"

âœ… **Solution:**
```bash
# Verify API key is valid
# Check network connection
# Review logs: tail -50 output/logs/ingestion.log
# Run step 1 again: python -m model.ingestion.fetch_data
```

### âŒ "Spark write failed (Hadoop)"

âœ… **Solution:**
The pipeline automatically falls back to Pandas/PyArrow.
If issues persist:
- Option A: Download winutils.exe â†’ `C:/hadoop/bin/`
- Option B: Run on Linux/WSL instead

### âŒ "Memory error during Spark"

âœ… **Solution:**
```bash
# Close other applications
# Increase available memory
export SPARK_DRIVER_MEMORY=4g
python main.py
```

### âŒ "Module not found" errors

âœ… **Solution:**
```bash
# Verify virtual environment is activated
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Reinstall dependencies
pip install --upgrade -r requirements.txt
```

### âŒ "Rate limit (429 error)"

âœ… **Solution:**
```bash
# The pipeline retries automatically with exponential backoff
# If still failing after 5 min:
# 1. Wait 15-20 minutes
# 2. Check TMDB API status
# 3. Reduce INGESTION_CONCURRENCY to 3
```

---

## ğŸ“š Technology Stack

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Distributed Processing** | Apache Spark | 4.1+ | ETL at scale |
| **Data Manipulation** | Pandas | 2.0+ | Data analysis |
| **Visualization** | Matplotlib + Seaborn | 3.7+ / 0.13+ | Plots |
| **API Client** | aiohttp | 3.8+ | Async HTTP |
| **Column Format** | PyArrow | 13.0+ | Parquet I/O |
| **Configuration** | python-dotenv | 1.0+ | Environment vars |

---

## ğŸ“ What You'll Learn

### Data Engineering
âœ“ Async API clients with rate limiting  
âœ“ ETL pipeline design & orchestration  
âœ“ Schema validation & enforcement  
âœ“ Error handling & retry logic  
âœ“ Data partitioning strategies  

### Apache Spark
âœ“ DataFrame transformations  
âœ“ Nested data flattening  
âœ“ Schema management  
âœ“ Aggregations & window functions  
âœ“ Platform compatibility  

### Analytics
âœ“ KPI calculation & reporting  
âœ“ Advanced filtering  
âœ“ Comparative analysis  
âœ“ Trend identification  

### Visualization
âœ“ Multi-plot figure composition  
âœ“ Statistical plots  
âœ“ High-resolution export  

---

## ğŸ“„ Project Outputs Explained

### KPI Report (`output/kpi_analysis.txt`)

Contains 7 analysis sections:
1. **General Statistics** - Dataset overview
2. **Top Performing** - Revenue, profit, ROI leaders
3. **Ratings** - Audience & critic analysis
4. **Advanced Queries** - Complex filters
5. **Franchise Analysis** - Performance comparison
6. **Top Franchises** - Series rankings
7. **Top Directors** - Director metrics

### Visualizations (`output/plots/`)

8 high-resolution PNG files for presentations/reports

### Interactive Notebook (`notebooks/analysis.ipynb`)

Run custom analysis, generate adhoc plots, explore data interactively

---

## âœ¨ Future Enhancements

- [ ] Machine learning revenue prediction
- [ ] Cloud deployment (AWS, GCP)
- [ ] Real-time data streaming
- [ ] Interactive Plotly dashboard
- [ ] Automated data quality checks
- [ ] Additional data sources

---

## ğŸ“ Support & Questions

1. Check logs in `output/logs/`
2. Review [Troubleshooting](#-troubleshooting) section
3. Verify all [prerequisites](#-prerequisites) installed
4. Check environment variables in `.env`

---

## ğŸ“„ License

Educational project for demonstrating data engineering best practices.

---

## ğŸš€ Ready to Start?

```bash
python main.py
```

**Questions?** Review the documentation sections above or check the logs for detailed error messages.

---

**Last Updated:** February 2026  
**Spark Version:** 3.5+  
**Python Version:** 3.8+  
**Status:** âœ… Production Ready
